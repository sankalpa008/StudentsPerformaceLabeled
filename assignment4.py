# -*- coding: utf-8 -*-
"""Assignment4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/148bdHgazEq-08kb3Rfte3517y1ge9Hy2
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time
import numpy as np

# Load the data
df = pd.read_csv('/content/sample_data/StudentsPerformanceLabeled-3.csv')

# Show basic information
print("Dataset Preview:")
print(df.head())
print("\nDataset Info:")
print(df.dtypes)

# Prepare features and target
X = df.drop('performance', axis=1)
y = df['performance']

# Use simple Label Encoding instead of OneHotEncoder
le = LabelEncoder()
for column in X.select_dtypes(include=['object']).columns:
    X[column] = le.fit_transform(X[column])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply label encoding separately on train and test sets
for column in X_train.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X_train[column] = le.fit_transform(X_train[column])
    X_test[column] = le.transform(X_test[column])  # Use transform only, not fit_transform

# Use GaussianNB instead of CategoricalNB (simpler to use with mixed data types)
model = GaussianNB()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate metrics (for multi-class classification)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Display results
print(f"\nResults at {time.strftime('%Y-%m-%d %H:%M:%S')}:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# Feature importance for multi-class: max difference between class means
feature_names = X.columns
theta_diff = np.max(model.theta_, axis=0) - np.min(model.theta_, axis=0)
feature_importance = dict(zip(feature_names, theta_diff))

print("\nFeature Importance (max difference between class means):")
for feature, importance in sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True):
    print(f"{feature}: {importance:.4f}")